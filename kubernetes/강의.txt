Kubernetes 강의교육

* Day 1

- LXC(Linux Container): 
	- chroot: 최상위폴더 변경. 가상공간을 생성해줌.
	- namespace: 프로세스 별 리소스를 따로 사용. 네트워크할당해줌
	- cgroup: 프로세스 별 가용 컴퓨팅 시스템을 분리. 자원할당해줌
	
- Pod: 컨테이너를 하나이상을 모아 넣은 것.

- Orachestration 도구: 분산 클러스트화된 환경에서의 서비스를 운영관리하는 플랫폼(컨테이너의 배포, 관리, 확장, 네트워킹 자동화) (PaaS)

- k8s goal: Desired State Management(사용자가 정의한 구성에 맞춰 사용자가 기대하는 상태로 동작하도록 안정적으로 유지해주는 것)

- yaml 기본틀
	apiVersion:
	kind:
	metadata:
		name:
	spec:

-vm 생성
	- master node은 최소 4g 이상, worker node은 3g 이상 vm 생성
		master node (=control plane): 관리 영역, no Pod
		worker node: pod(application workload) 생성

	- centos 실행을 위해 windows 기능에서 가상머신플랫폼 체크해제해줌. (이것때문에 wsl 가상머신이 돌아가지 않음)
		linux용 windows 하위 시스템 체크
	- virtual box sms 6.0.4버전
	- centos login: root/k8spass#
	- vm 네트워크:
		어댑터1 NAT 는 외부네트워크 (external)
		어댑터2 호스트 전용어댑터 는 호스트 전용(internal)->infral
	master node: 192.168.56.100
	node1: 192.168.56.101
	node2: 192.168.56.102
	node3: 192.168.56.103

- kubernetes 구축 
  1. vm 설치
  2. centos 설치 및 환경구성
  3. docker-ce + kubernetes 설치
  4. node 구성을 위한 복제 (master vm을 토대로 worker vm 3개 복제)
  5. matser + node1 + node2 구성 -> init -> join
  6. 네트워크 plugin 추가
  7. dashboard 연결 -> secure, CA(인증서)
  
- kubernetes 를 위한 사전 OS 구성(master node)
  - 보안 구성 OFF: SELinux down(permissive) / firewalld-cmd(방화벽) 해제
			# vi /etc/selinux/config
				SELINUX=permissive 변경: selinux down
			# setenforce 0
			# sestatus: permissive 바뀐 것 확인
			# systemctl stop firewalld && systemctl disable firewalld: 방화벽 해제
				설치 이후에는 방화벽 재구성을 따로해야함.
  - container(process)가 반드시 physical memory에서만 구동: swap off
			# swapoff -a
			# vi /etc/fstab
				#/dev/mapper/centos_k8s--master-swap swap 주석처리
  - network packet
			# cat /proc/sys/net/ipv4/ip_forward: 1이 출력되야됨. 아니라면 node끼리 join 할 때 에러남.
  - iptables가 bridge 된 트래픽 인식하게 하기
			# modprobe br_netfilter
			# lsmod | grep br_netfilter: 확인
			# vi /etc/modules-load.d/k8s.conf
				br_netfilter 입력저장
			# vi /etc/sysctl.d/k8s.conf
				net.bridge.bridge-nf-call-ip6tables = 1
				net.bridge.bridge-nf-call-iptables = 1 입력저장
			# sysctl --system /etc/sysctl.d/k8s.conf: 확인

- xfs 가 ext4 보다 권장사항. 속도가 빠름.
- yum: /etc/yum.repos.d/
- apt: /etc/apt/sources.list
- cgroup의 상위 데몬 systemd. k8s는 cgroup 사용안함.

- 도커설치는 yum 디렉토리에 docker.repo 파일 만들고 yum update 한 후 docker service를 올리면 도커설치완료
	# systemctl status docker: 도커 상태
- kubernetes 설치도 kubernetes.repo 파일 만들고 서비스올리면됨.
	- GPG: GNU Package GUIDE. 패키지 보안 인증 

- kubernetes tools
	1) kubeadm init -> k8s cluster bootstrap 초기화
	   kubeadm upgrade
	2) kubectl -> api calls 시 사용. Pod 동의 api-resource 생성/관리/삭제 시에도 사용.
	3) kubelet -> 모든 노드에 설치, 노드 간 통신
		
	* kubeadm init 할 때 [error cri] 에러 해결:  **kube 1.24 버전부터는 에러가남. 1.23은 에러안남.
		# rm /etc/containerd/config.toml
		# systemctl restart containerd 
		명령어 진행 후 다시 init 실행. kubeadm join도 같은 에러나서 위 솔루션으로 해결함
	
# kubeadm init --pod-network-cidr=10.96.0.0/12 --apiserver-advertise-address=192.168.56.100
	[init] Using Kubernetes version: v1.24.4
	[preflight] Running pre-flight checks
		(생략)
	Your Kubernetes control-plane has initialized successfully!

	To start using your cluster, you need to run the following as a regular user:

	  mkdir -p $HOME/.kube
	  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	  sudo chown $(id -u):$(id -g) $HOME/.kube/config

	Alternatively, if you are the root user, you can run:

	  export KUBECONFIG=/etc/kubernetes/admin.conf

	You should now deploy a pod network to the cluster.
	Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
	  https://kubernetes.io/docs/concepts/cluster-administration/addons/

	Then you can join any number of worker nodes by running the following on each as root:

	kubeadm join 192.168.56.100:6443 --token m1coue.w4kouj5439pc78as \
        --discovery-token-ca-cert-hash sha256:a6249bc1644ae598fec14c2b0f7a18e4ce43bc7f2012ee0ba8fd97e6792a832c
		
	- 위 명령어 4개를 입력하고 마지막에 kubeadm join을 node1,2 머신에 명령어입력하면 join 됨.
		근데 join 명령어 전에 각 node 머신에 # ssh [master 머신 hostname] 으로 연결부터 해준다. master에서도 연결해준다

	# yum repolist: 설치된 라이브러리확인
	# kubectl get nodes: run 되고 있는 node (join 확인) - calico가 올라가야 notready 상태에서 ready로 변함

- Calico : 가상머신이나 컨테이너를 위한 네트워킹, IP 관리, 접근 제어, 모니터링 등 다양한 네트워크 관련 기능을 제공하는 오픈소스 프로젝트이다.

- calico 설치
# curl https://docs.projectcalico.org/manifests/calico.yaml -O
# kubectl apply -f calico.yaml: yaml을 apply 하는 것은 pod를 생성하는 것.
# kubectl get pod --all-namespaces

- kubectl 자동완성 추가
보통 yum bash-completion 이 자동완성으로 설치가 되어있는데 kubectl에는 다시 적용시켜줘야함.
# source <(kubectl completion bash)
# echo "source <(kubectl completion bash)" >> ~/.bashrc 

- alias
# alias k='kubectl'
# alias d='docker' 
등으로 명령어 줄일수 있음


- dashboard start 방법
	https://github.com/kubernetes/dashboard/releases: kubectl version 에 맞게 installation 명령어 복붙해서 입력
		
	# kubectl get pod --all-namespaces: kubernetes-dashboard 2개 올라온거 확인
	
	1) proxy(8001) : GKE에서 사용
	2) apiserver(6443) : host server에서 사용
	3) NodePort(30000~32767) : 외부 연결용
	
	# kubectl cluster-info : dashboard 연결주소 -> 보안 인증, 권한설정을 해야 접근가능.
		https://192.168.56.100:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
		아래처럼 변경해야함
		https://192.168.56.100:6443/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy

	# kubectl describe pod [NAME] -n [NAMESPACE]:
		kubectl get pod --all-namespaces 할 때 나오는 거 이용해서 사용.
	# kubectl describe node [vm명]: ex.kubectl describe node k8s-master
	# kubectl -n [NAMESPACE] get secrets
	# kubectl -n [NAMESPACE] describe secrets [NAME]: dashboard login 시 필요한 토큰 조회
	
	# kubectl api-resources: 약칭
	
* Day 2

- apiserver 를 이용한 dashboard 구성(+ cacert)
	1) 현재 설치한 대시보드에 권한 설정: clusterrole -> verb=create 식으로 옵션을 주면됨.
	2) clusterrole을 사용할 수 있는 계정: serviceaccount(프로세스를 제어하기 위한 계정 -> 프로세스는 즉, container)
	3) clusterrolebinding: clusterrole + serviceaccount 를 binding
	4) openssl을 이욯나 인증서 생성: 리눅스에 있는 인증서를 windows로 옮길거임
	5) certutil.exe(컴퓨터 인증서관리)를 이용해서 certmgr.msc에 등록
	6) 크롬 시크릿모드에 인증서와 함께 dashboard 연결
	7) token access 진행

	- 시작
	# kubectl get clusterrole: role list 
	# kubectl describe clusterrole [role name]: 해당 role에 대한 권한확인
	
	# kubectl get pod --all-namespaces: namespace list
	# kubectl get sa(serviceaccount) -n [namespace 명]: 해당 namespace에 대한 serviceaccount 확인
	# kubectl describe -n [namespace 명] sa [namespace 명]: 더 자세한 sa 확인
	
	# vi ClusterRoleBinding.yaml: 1)과 2) 작업을 binding
		apiVersion: rbac.authorization.k8s.io/v1
		kind: ClusterRoleBinding
		metadata:
		  name: kubernetes-dashboard2
		  labels:
			k8s-app: kubernetes-dashboard
		roleRef:
		  apiGroup: rbac.authorization.k8s.io
		  kind: ClusterRole
		  name: cluster-admin
		subjects:
		- kind: ServiceAccount
		  name: kubernetes-dashboard
		  namespace: kubernetes-dashboard
	# kubectl apply -f ClusterRoleBinding.yaml
	
	# grep 'client-certificate-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d >> kubecfg.crt: 인증서만들기
	# grep 'client-key-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d >> kubecfg.key: 인증서 만들기
	# openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name "kubernetes-admin"
	# cp /etc/kubernetes/pki/ca.crt .

	ftp 프로그램(파일질라, wincp) 로 dashboard 자체 폴더를 윈도우로 옮겨주기
	
	> window powerShell 관리자모드
		# certutil.exe -addstore "Root" [dashboard폴더 경로]\ca.crt
		# certutil.exe -p [비밀번호] -user -importPFX [dashboard폴더 경로]/kubecfg.pl2
		# certmgr.msc: 컴퓨터 인증서확인 창에서 인증서 확인
		
	- centos dashboard token:
		https://192.168.56.100:6443/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy
	
	- 위 192.168.56.100 url 들어가서 쿠퍼네티스창 확인
		token 값구하기:
			# kubectl get pod --all-namespaces: namespace 명이 kubernetes-dashboard 인 것 찾고 확인
			# kubectl -n [namespace명] describe secrets: namespace 명은 위에서 찾은 list 중 kubernetes-dashboard 가 된다.

			eyJhbGciOiJSUzI1NiIsImtpZCI6IjI1ZWozY1IxcGo5TlpOQm1VS0lGQzRNRjhVZEFMVHMwRTFUOTZmX2N1eWMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi10d3hnNiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImI4OTE0MTU2LTQ3ZWQtNDIxNC05NGI0LWE5OTBmZTQ5ZTc1MSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.wP6Q7eKInOlr_ekiwYff5AtY-JosfupqKRT7WSt2eaxc0vhFxm6esRTKS_NWavFjbBDoJJ-M8i1tUHkiXIfC0ZvTlikgA-Hi7lWPNq49VVhqv2nH3-5T6yRlu7req6yS_Cz2x3oFbdKRLdE1hzKIwogN90xgH5lmYusCNaAAMA_2aTdQfSXxYGLieUu05bYT-PBpWBFbX7JKjPKgdud7b83dkuUo6ihBe05WnyokE5qiojnM0SY-gQguiAE7SZ5LB1FgiCVEJ2WcpalVJWzoBKELhihndh90qSDmX2An_ADYF3gAyLDh9UPJzzt9k8c-F4OxBI6WnEsP1GqpHVngZQ

- Prometheus+grafana
	# rdate -s time.bora.net: 시간 동기화- 모든 노드에 적용
	# kubectl create namespace monitoring: 그룹추가
	# kubectl get ns: namespace 조회
	yaml 파일 만드고 
	# kubectl create -f [경로/파일명] : yaml 파일마다 해주기

	port
	30003 prometheus
	30004 grafana
	
- GKE( google kubernetes engine) - GCP 기반

	- gcp 기반으로 kubernetes를 구축하면 master vm을 따로 생성안하고 cluster에서 자동으로 잡아줌.
		그래서 node vm을 만들어주지 않아도됨.
		
	- GCP 기반의 GKE 구축
		1) GCP 가입
		2) GCP project 생성 후 project에서 GKE 구성
		3) GKE 설정 및 GKE 생성
		4) plugin 설치(dashboard)
		5) dashboard 접근
		
	- GCP dashboard token:
		https://localhost:8080/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy
	
- taint가 디폴트로 걸려있기 때문에 master에는 pod가 올라가지 않음. taint를 빼면 올라갈 수 있음.
	# kubectl describe nodes | grep -i taint

- kubernetes components(pod 생성과정 동작방식)_그림참고 (architecture.png, arch_seq_diagram.png)
	- Scheduler:  사용자가 요청했을 때 어느 node에 pod를 붙일 것인지 결정. 효율성(matric 기준)이 낮은 곳에 pod를 붙임(binding)
	- 순서:
		1) 사용자 요청을 받는다. (kubectl apply -f *.yaml 명령어 입력)
		2) API Server가 무조건 받는다.
		3) API Server는 DB(etcd)에 기록한다.
		4) etcd는 기록이 완료되었다고 API Server에 전달함.
		5) API Server는 pod배치할 node를 정해달라고 Scheduler로 요청함.
		6) Scheduler는 효율성을 따져서 배치할 node를 선택한다.
		7) Scheduler는 선택한 node를 API Server로 회신한다.
		8) API Server는 선택된 노드의 Kubelet으로 전달한다.
		9) Kubelet은 해당 노드의 Container Runtime 으로 전송한다. 즉 docker run이 실행됨.
		10) Kubelet은 그 pod를 뒤집어 씌어서 pod 생성을 완료한다.
		11) Kubelet은 성공적으로 완성하면 API Server로 회신한다.
		12) API Server는 etcd에 상태정보(success, fail 등)를 기록한다.

	- Master nod: 클러스터에 있는 노드들과 통신하여 desire state를 유지하기 위한 제어를 수행한다.
		- desire state: 만약 ReplicaSet controller 에 replicas = 3 개로 설정하고 pod가 3개가 존재한다.
			근데 3번째 pod가 장애가 나면 3개를 유지해야하기 때문에 pod를 한개 더 생성함.
		- 주요 구성요소:
					-> etcd: k8s의 db(json 구조). 상황/상태 기록
						유지관리 - snapshot save|restore [snapshot 경로]
					-> apiserver: 전체 클러스터 관리
					-> scheduler
					-> controller: multi-pod
						workload resource: Deployment, ReplicaSet(RS=RC), DaemonSet, Job, CronJob
					-> kubelet: 노드 간 통신
					-> kube-proxy: 클러스트 내의 pod network 지원
	- Worker node: workload를 수행하기 위한 object resource를 생성/운영하는 노드
		-주요 구성요소:
					-> kubelet
					-> kube-proxy: 모든 요청(트래픽)에 대한 포워딩
						vs. docker-proxy: NAT, NAPT
					
	- kube-proxy: pod 내에서의 클러스터 통신담당
		kubelet ~ Container Runtime(Docker)까지 node. node에 kube-proxy 존재.
		10번에서 pod 생성이 완료되면 그 때부터 kube-proxy가 통신을 하는 것.
		
# kubectl {apply | create | delete | get | describe} -f *.yaml: pod 생성
	- 파일명이 아닌 NAME 이름으로도 가능 
		# kubectl get deploy,po,svc 해서 나온 NAME 값
		# kubectl delete [pod | svc | deploy] [NAME]
	- apply: 생성후 update 가능
	- create : 생성만
	- delete : pod 정상종료  (# kubectl delete -f *.yaml --grace-period=0 --force : 강제종료, 비정상종료)
	- get: 기본조회
	- describe: 세부조회

[실습] <node-app 폴더>
	app.js 라는 서비스를 pod생성해서 실행시킬거임

	# kubectl get pod -o wide (--show-labels): 어떤 노드로 배치되어있는지 ip가 뭔지 자세히 알 수 있음.

	- 순서
	app.js: application service source
	Dockerfile: 이미지 제작
	mynode2-pod.yaml: pod 생성
	mynode2-svc.yaml: pod 생성
	yaml file apply명령
	
	# kubectl get pod,svc -o wide --show-labels: 잘 올라갔는지 확인
		CLUSTER-IP 는 내부에서만 접속가능
		EXTERNAL-IP 는 내,외부에서 접속가능
	# kubectl get pod -o wide -l app: label에서 key가 app이라는 pod만 조회

	mynode2-pod.yaml 만 pod 생성 시 scheduler가 자동으로 node vm에 배치해준다.
	node vm이나 master node에서는 배치된 node vm의 ip로 접속하여 서비스를 돌릴 수 있지만, 외부에서는 접근되지 않는다.
	그래서 mynode2-svc.yaml 로 pod 생성해서 서비스를 연결시켜주면 외부에서도 접속이 가능하다.
		
	- 위처럼 yaml 파일을 2개 생성해서 apply 명령어를 두번쳤지만 하나의 파일로 여러 pod를 생성할 수 있다. mynode.yaml 가 예시.
		apiVersion: v1
		kind: Pod
		metadata: 
		  name: mynode2-pod
		  labels:
			app: mynode
		spec:
		  containers:
		  - image: leehorim/mynode:2.0
			name: mynode-containers
			ports:
			- containerPort: 8000
		---
		apiVersion: v1
		kind: Service
		metadata: 
		  name: mynode2-svc
		spec:
		  selector:
			app: mynode
		  ports:
		  - port: 9000
			targetPort: 8000
			protocol: TCP
		  type: LoadBalancer (GCP 가 아닐 때는  externalIPs:  - 192.168.56.101)
		
		맨 마지막줄에 externalIPs key로 외부IP를 직접지정했다.
		근데 위처럼 자동으로 줄 수 있는데 제한사항이 존재함.
			centos 내부에서 명령어를 내리면 에러가 생김. 구글같은 클라우드 레벨에서만 가능하다. 이 뜻은 GCP를 이용해야함.

- deployment: pod의 상위개념 <my-deploy 폴더>
	replicas 는 당연 가지고 있고, 여기에 rolling update, rollout 존재(pod 무중단으로 update 등이 가능함).

	# kubectl api-resources | grep -i deployment: 에서 apps/v1 확인
	# kubectl create namespace my-ns1: namespace 생성
	# vi my-deploy.yaml
		apiVersion: apps/v1
		kind: Deployment
		metadata: 
		  name: my-deploy
		  namespace: my-ns1
		  labels:
			run: my-app
		spec:
		  replicas: 3
		  selector:
			matchLabels:
			  run: my-app
		  template:
			metadata:
			  labels:
				run: my-app
			spec:
			  containers:
			  - image: nginx:1.21
				name: my-nginx-containers
				ports:
				- containerPort: 80
		---
		apiVersion: v1
		kind: Service
		metadata: 
		  name: my-svc
		  namespace: my-ns1
		spec:
		  selector:
			run: my-app
		  ports:
		  - port: 8001
			targetPort: 80
			protocol: TCP
		
	# kubectl apply -f my-deploy.yaml
	# kubectl get -n my-ns1 deploy,svc,po -o wide: namespace가 잡혀있으니 -n 옵션 걸어주기
	# kubectl delete -f my-ns1 [NAME]: pod 하나 종료
	
	결론)
	다시 kubectl get 으로 조회해보면 삭제된 pod는 나오지 않고 새로운 NAME의 pod가 생성되어 조회된다.
	결국 하나를 삭제해도 3개가 유지되도록하는 deploy
	
* Day 3

- Pod: k8s 최소 application 배포 단위. container를 담는 그릇(=host)
	- container 종류:
		1) runtime container 
		2) init container -> pod 기동시점에 우선 처리되야 하는 조건 container. 
						  -> 내부,외부 조건이 부합되면 같이 있는 container를 처리하고 terminated(삭제가됨)
		3) SideCar container -> 로그 수집, 모니터링과 같은 보조적 역할을 하는 container
		
	- 초기 pod 가 생성 될 때 Pod를 만드는 pause(=infra) container를 먼저 생성: 
		network 기능(PodIP, Mac, ..) 배포 즉, network namespace 
		-> 한 pod의 two container는 통신은 localhost:port 로.
			apiVersion: v1
			kind: Pod
			metadata:
			  name: mynode2-pod
			  labels:
				app: mynode
			spec:
			  containers:
			  - image: leehorim/mynode:2.0
				name: mynode-containers
				ports:
				- containerPort: 8000
			  - image: leehorim/mynode:2.0
				name: mynode-containers2
				ports:
				- containerPort: 9000
	- Pod network namespace: Linux network stack을 가상화한 것.
	  격리형 공간을 생성하고 network 기능 배포
	  routing table
	
[multi pod container 에서 container 간 통신 - 동일 node에서 pod 간의 통신]
	yaml 파일을 위처럼 컨테이너 2개를 작성해서 대쉬보드 uri > 상단 + 버튼에서 직접 업로드 
	대쉬보드에 kubectl이 내장되어 있어서 바로 pod 생성이 가능하고, 2개의 컨테이너 내부로 들어가는 cli 입력창도 가능하다.
		apiVersion: v1
		kind: Pod
		metadata:
		  name: multi-pod
		spec:
		  containers:
		  - image: dbgurum/k8s-lab:p8000
			name: one-containers
			ports:

			- containerPort: 8000
		  - image: dbgurum/k8s-lab:p8080
			name: two-containers
			ports:
			- containerPort: 8080
	
파드
	- 복수의 컨테이너 포트들은 중복되면 안됨.
	
	-대쉬보드가 아닌 ssh에서의 명령어:
	  # kubectl get po: 대쉬보드로 올린 pod 조회
	  # kubectl exec multi-pod -c one-container -- curl localhost:8080 
		컨테이너가 2개 이상일 때는 -c 옵션을 붙여야 터미널 접속이 가능
	  # kubectl get po -o wide: IP 도 같이 조회되는데 이는 PodIP. localhost와 같다.

- 다른 node 간 pod 통신: calico 를 통해 통신. 

- Label: pod | Deployment와 service를 연결
	1) 식별
	2) 그룹(집합) 구성
	3) key:value 구조, 하나 이상 설정 가능->선택
	
	- label과 유사한 Annotations: 추가적인 데이터 저장 방법, 식별 방법 등 제공, 스케쥴링(node 지정)..
	
	[label 실습]
		1. namespace crate
		2. service 생성
		3. pod 3개
		4. pod 당 다수의 container
	
		label 폴더의 label-1.yaml 참고
		
	[3번 label 실습]
		label-choice 폴더의 label-2.yaml 참고
		
		label 은 key: value 형식으로 여러개 설정가능하다.
		service를 여러개 구현해서 label에 맞는 서비스를 선택해서 pod를 생성한다.
		
- node schedule
	- kube-scheduler-k8s-master 사용중
	- pod를 생성하면 scheduler에 의해 노드에 할당- 각 노드의 metric(resource usage: cpu, memory) 정보를 추적
	- 기본값은 자동으로 할당해줌
	- 수동할당: spec.nodeSelector, spec.nodeName 를 yaml 파일에 넣어주면됨.
		schedule 폴더의 nodesel-pod1.yaml 참고
		nodeSelector value를 master node로 지정하면 에러(pending)남. taint 때문에.
		만약 master node에도 지정하고 싶다면 yaml의 spec.tolerations 를 지정하면 가능하다.
		
		1) nodeSelector: 
			# kubectl describe no k8s-node1 에서 조회된 값 "kubernetes.io/hostname=k8s-node1" 입력
		2) nodeName:
			"k8s-node1" 입력. 권장하지 않음
		3) node label: 현재 보유한 노드 수가 많을 때 특징을 기재하는 것.
			ex. 어떤노드(gpu), 어떤노드(ssh), 어떤노드(특징기재)
			# kubectl label node k8s-node1 disktype=ssd: 라벨을 건다.
				spec.nodeSelector: 
				  disktype: ssd 
				처럼 yaml 파일 짜면 된다.
			
- namespace
	# kubectl config set-context --current --namespace=[NAMESPACE]: namespace 기본값 변경
	# kubectl config view: 현재 namespace 조회
	
	- vi kubens 실행파일로 간단하게 namespace를 변경 조회할 수 있음(namespace 폴더의 kubens 파일)
		# vi kubens
		# chmod 744 kubens
		# ./kubens: 모든 namespace 와 현재 namespace 출력
		# ./kubens [NAMESPACE]: namespace 변경
		# ./kubens -: default 변경
	
- kompose: docker-compose.yml 과 k8s.yaml을 호환가능하게 하는 도구(convert).	
	
	kompose 폴더의 docker-compose.yml 파일 참고
	
	-docker-compose install
	# curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
	# chmod +x /usr/local/bin/docker-compose
	# ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose

	# vi docker-compose.yml
	# curl -L http://github.com/kubernetes/kompose/releases/download/v1.22.0/kompose-linux-adm64 -o kompose
	# chmod +x kompose
	# echo $PATH
	# mv kompose /usr/local/bin/kompose

	# kompose convert: compose.yml 파일을 자동으로 kompose.ymal 파일로 변환됨
	# kubectl apply -f [파일들]: 파일 한번에 쓸 때는 콤마(,)로 구분하고 띄어쓰기 없이

- Service: Pod에 연결성을 부여. label로 연결
	- spec.type:
		1) ClusterIP: 내부용
		2) NodePort: 외부용. L4, 30000~32767 port pool(random할당, 지정가능)
		3) LoadBalancer: 외부용, IP할당(ExternalIP) -> cloud(GKE)에서 public IP 할당
			-> CentOs에서는 MetalLB(bearMetal LB) 구축하면 가능
		4) Ingress: 외부용. L7(http, https)
		5) NetworkPolicy: 송신(egress),수신(inress) 정책 구현
	
	- why needs? 장애 시 application 코드 수정을 매번 해야하는 부담을 없앨 수 있다.
	- Service 구조?
		1) Pod, Service를 label로 연결 생성 -> 이름/IP가 kube-dns에 자동으로 등록
		   연결이 된 후, 트래픽 요청이 들어오면 apiserver에 등록. apiserver는 kube-proxy랑 연결됨
		2) 요청 -> Service -> Pod -> container -> app
						  ㄴNAT(IP변환)
		3) 다른 노드의 pod 간 통신: calico
		4) Service 삭제 시 자동으로 apiserver, kube-dns에서도 삭제됨.
		
		
	- ClusterIP
		[ClusterIP 실습]
		cluster 폴더의 clusterip-1.yaml 참고
		pod, service 생성 실행 후
		# kubectl get svc clusterip-svc -o yaml: 
			internalTrafficPolicy: Cluster 를 볼수 있는데 이게 여러개의 pod 연결할 때 load balancing 정책임.
		
	- NodePort: 외부에서 접근가능한 Port 할당
		yaml 파일 생성시 spec.type을 NodePort로 지정하면 pool 에서 랜덤으로 할당됨. 모든 노드의 해당 포트가 open이 됨
		만약 지정하고 싶다면 spec.ports.nodePort로 지정.
		# netstat -nlp | grep [port번호]: 열린 포트 확인(각 노드에 명령어 쳐보면 다 열려있음)

			apiVersion: v1
			kind: Pod
			metadata:
			  name: pod-a
			  labels:
				app: mynode
			spec:
			  containers:
			  - image: dbgurum/k8s-lab:v1.0
				name: pod-c-containers
				ports:
				- containerPort: 8000
			---
			apiVersion: v1
			kind: Service
			metadata:
			  name: svc
			spec:
			  type: NodePort
			  selector:
				app: mynode
			  ports:
			  - port: 8001				# service
				targetPort: 8000		# container
				nodePort: 30001			# OS, nodePort 를 지정하지 않으면 랜덤으로 지정됨.
		
		- 외부에서 내부로 접근하는 포트 순서: 30001 -> 8001 -> 8000
		- 모든 노드 IP에서 접근이 가능하고, 외부에서도 마찬가지로 접근가능.
		- 트래픽 분산 효과: pod2개에 서비스1를 연결해서 nodePort를 지정해주면 트래픽요청이 분산되어 pod에 접근한다.
					즉, 192.168.56.101:30001 로 접속하게 되면 pod 2개 중 효율적인 pod 로 트래픽이 요청된다.
	
		# kubectl edit svc [service명]: externalTrafficPolicy, internalTrafficPolicy 가 Cluster로 되있는 걸 확인 할 수 있다.
				externalTrafficPolicy, internalTrafficPolicy 를 Local 로 바꾼다면
				트래픽이 분산되어 나가지 않는다.
		
	- LoadBalancer: 외부에서 접근가능한 IP 할당
		- GCP 로 실습
			** # kubectl api-resource: 하면 만들려는 파드나 서비스등의 apiVersion, kind 를 알 수 있다.
			apiVersion: v1
			kind: Pod
			metadata:
			  name: pod-lb
			  labels:
				run: lbapp
			spec:
			  containers:
			  - image: dbgurum/k8s-lab:v1.0
				name: lb-containers
				ports:
				- containerPort: 8000
			---
			apiVersion: v1
			kind: Service
			metadata:
			  name: my-svc
			spec:
			  type: LoadBalancer
			  selector:
				run: lbapp
			  ports:
			  - port: 9090		
				targetPort: 8080
	
		- 실습 2: deploy lb
			lb-deploy 폴더의 deploy-app.yaml 참고
			
	- Ingress: http, https 연결.
		- 사진 ingress.png
		- routing rule: cluster에 진입점. 어떤 서비스를 타서 어떤 pod로 가라는 경로 배정

		[실습]
		서비스 3개에 각 서비스 마다 pod 1대. 서비스yaml에 노드포를 지정하는 게 아닌 ingress controller에서 자동으로 부여해줌
		ingress 폴더의 webapp.yaml, webapp-ing.yaml 참고
		
		https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal-clusters 에서 베어메탈
		# kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.3.0/deploy/static/provider/baremetal/deploy.yaml
		# kubectl get po --all-namespaces
			ingress-nginx          ingress-nginx-admission-create-6jmrh         0/1     Completed   0                   64s
			ingress-nginx          ingress-nginx-admission-patch-hxzqj          0/1     Completed   2                   64s
			ingress-nginx          ingress-nginx-controller-77cb5dbf4d-bndks    1/1     Running     0                   64s
	
		ingress 는 label이 아닌 annotation 을 사용
		
		webapp.yaml, webapp-ing.yaml apply 후
			-> webapp-ing.yaml 을 apply 할 경우 에러가날거임(ingress 가 update<v23>되고 발생한 오류임)
			-> # kubectl delete validatingwebhookconfiguration ingress-nginx-admission: 인증과 관련된 ingress 설정 제거
		
		# kubectl get ing,pod,svc -o wide: 어떤 노드에 붙었는지 IP 확인

		# kubewctl get pod --all-namespaces: NAMESPACE 확인(ingress-nginx)
		# kubectl get ing,pod,svc -o wide -n [NAMESPACE]: 포트 확인후 접속. NAMESPACE=ingress-nginx
		
		# curl [IP]:[PORT]/[PATH]
		
* Day 4

- docker volume:
	- container의 생명주기와 같다. container가 생성/삭제되면 생성/삭제된다.
	- bind mount: 절대경로
	- volume: /var/lib/docker/volume
	- tmpfs: 임시 메모리공간
	
- kubernetes volume:
	- Pod 생명주기와 같다. Pod가 생성/삭제되면 생성/삭제된다.
	- pod 안에 볼륨이 생성된다.
	- 그 볼륨은 컨테이너와 mount 된다.
	- pod 당 볼륨 갯수는 제한이 없다.
	- 하나의 볼륨으로 여러 컨테이너가 공유할 수 있다.
	- 컨테이너마다 독립적인 볼륨을 가질 수 있다.
	- 하나의 컨테이너는 공유볼륨과 독립볼륨을 동시에 가질 수 있다.
	
	- spec.volumes.type
		1) emptyDir: pod내의 container 간의 공유 볼륨 mount-> sidecar container
			ㄴ non-persistent volumne(지속적이지 않음)
		2) hostPath: pod to host. 상호간의 데이터 교류.
			즉, pod 내의 볼륨과 pod 밖의 host경로를 mount. pod가 망가져도 데이터는 존재함.
		3) NFS: 노드끼리 host 경로를 mount해줌. node가 망가져도 데이터 존재함.
		4) PV(admin) & PVC(developer): OS 의존성 x
			persistent volume & persistent volume claims
		(2),(3) 은 의존성이 너무 강해서 잘안씀
		
		[emptyDir 실습]-공유볼륨
		pod 내의 공유되는 볼륨은 추상적이다. 사실상 컨테이너끼리 mount되는 느낌이랑 비슷.
		
		vol-test 폴더의 vol-1-empty.yaml 참고
		
		# kubectl exec -it pod-vol-1 -c container1 --bash:
			내부들어가서 volume dir 존재확인하고 container2도 확인해서, 서로 연결되어있는지 파일생성해서 확인해보기
		# kubectl cp [pod명]:[mountPath]/[만들파일이름] -c [컨테이너명] [경로/파일이름]: 컨테이너1에 있는 볼륨의 파일 복사해서 가져오기
		# kubectl cp [경로/파일이름] [pod명]:[mountPath]/[만들파일이름] -c [컨테이너명]: 현재 파일을 컨테이너1에 있는 볼륨폴더에 집어넣기
		
		[sidecar 실습]
		특정 pod의 log 수집이나 monitoring 하는 기술
		
		vol-test 폴더의 vol-2-sidecar.yaml 참고
		
		확인작업
		# kubectl exec -it log-pod -c html-container -- /bin/cat /usr/share/nginx/html/index.html
		# kubectl exec -it log-pod -c sidecar-container -- /bin/cat /html-log/index.html
		
		[hostPath 실습]
		node1과 컨테이너 간 mount
		
		vol-test 폴더의 vol-3-hostPath.yaml 참고
		
		# kubectl exec -it pod-vol-2 -- bash
		
		[hostPath + NFS]
		GCP 사용
		vol-test 폴더의 weblog-pod2.yaml, mysql-pod.yaml 참고
		
		- PV, PVC:
			- PV: 클러스터에 볼륨으로서 추가되는 가상 스토리지 인스턴스
			- PVC: 특정 유형과 구성으로 지속적 스토리지를 프로비저닝(미리 만들어놓는다.)하는 요청
			
			- 대략적인 흐름
				1) admin 은 미리 PV를 만들어놓는다.(프로비저닝)
				2) developer 는 권한과 용량에 해당하는 PVC 를 생성한다.
				3) PVC의 권한과 용량이 매칭되는 PV를 자동으로 bind 된다.
				4) developer 는 매칭된 pvc를 확인하고 pvc를 이용해서 pod를 만든다.
			
			[실습]
			pv-pvc 폴더안의 pv1.yaml, pvc1.yaml, pod1.yaml 참고
			
			pv에는 3개, pvc는 4개를 만듬
			apply 하고 # kubectl get pv,pvc 했을 때 나오는 결과는
			pvc3번만 Pending이 됨. 이유는 용량과 권한에 해당하는 pv가 없어 bind 되지 않았기 때문임.
			# kubectl get pv,pvc -o wide: 누가 어디에 붙어있는지 자세히 보기
						

			pod1.yaml 에서 persistentVolumeClaim.claimName 으로 pvc 선택해서 apply
			
			확인
			# kubectl exec -it mynode-pod -- bash
			
			결론: node(host)와 pod mount 시 의존성없이 가능

			- spec.accessModes:
				- ReadWriteOnce(RWO): 하나의 노드가 볼륨을 R/W 가능하도록 mount
				- ReadOnlyMany(ROX): 여러 노드가 볼륨을 R/O 가능하도록 mount
				- ReadWriteMany(RWX): 여러 노드가 볼륨을 R/W 가능하도록 mount
				
				복수개가 적용될 수 있기 때문에 yaml 파일 만들 때 dash(-) 가 붙음
				
			- spec.persistentVolumeReclaimPolicy: pv의 공간을 회수하는 정책
				- Retain: 디폴트. 데이터 보존
				- Recycle: 데이터 삭제 후 재사용
				- Delete: 사용이 종료되면 해당 볼륨 삭제
				
			- spec.storageClassName:
				- 동적 볼륨
			
			- pvc 가 pv를 매칭할 때 자동이 아닌 수동으로 하는 법:
				- pv를 생성할 때 pv에 라벨이름을 부여한다.
					metadata.labels.name: pv4	(name은 label 처럼 type 이런거로 변경해도됨. pod 라벨이름 쓸 경우와만 맞추면됨)
				- pvc를 생성할 때 pv의 라벨이름을 적용한다.
					spec.selector.matchLabels.name: pv4
			
			[예제실습]
			- pv가 매칭되어 pvc를 선택한 후 pod가 만들어진 상태에서 pv 용량변경가능.
				pvc 도 가능하지만 제약사항이 많음.
			pv-pvc 폴더안의 pv-pvc-pod.yaml 참고
			
			# kubectl edit pv [pv 명]: [pv 명]=pv-volume
				spec.capacity.storage 변경저장. 변경작업 있으면 바로 반영됨
			# kubectl get pv,pvc,pod -o wide: 확인
			
		[동적볼륨 실습]
		GCP사용
		pv-pvc 폴더의 gce-sclass.yaml, gce-pvc-sclass.yaml 참고
		
		gce google shell 에서 apply 하기
		google 디스크에 생성된걸 확인할 수 있다.
		
		# gcloud compute disks delete [디스크 이름] --zone [zone 이름]: 디스크영구 삭제
			
- configmap & secret

	- 환경변수 설정, CMD에 args 설정, 구성파일(환경변수 모음)
	- key:value 구조
	- pod(deployment) 내에 참조됨
	- configmap 과 secret 차이점
		- configmap -> 기밀하지 않은 데이터 저장. 
		- secret -> 기밀한 데이터 저장. encoding(base64)으로 만듬. ex) MYSQL_ROOT_PASSWORD 
	- 1 mega 용량 제한
	- 구성파일(*.conf)
	
	[실습:proxy 역할로 구성변경하는 configmap 사용 방법]
		case 1) configmap create 후 pod yaml 작성 시 configmap 적용 코드 추가해서 apply
		# kubectl create configmap [cm 명] --from-literal=LOG_LEVEL=DEBUG: configmap 생성
			- [cm 명] = log-level-cm
			- cm == configmap
			- cm 설정하는 옵션
				--from-literal=[key=value] 또는 --from-literal [key=value]: 바로 create 하기
				--from-file=[파일명]: 미리 생성된 key:value 파일로 create하기
		# kubectl get cm
		# kubectl get cm log-level-cm -o yaml: yaml 로 보기
		# kubectl get cm log-level-cm -o yaml > [파일명.yaml]: yaml 로 떨구기
			json파일로 떨구고 싶다면 yaml 을 json으로 바꿔서 명령어 입력
		
		cm-1 폴더의 cm-pod1.yaml 참고
		
		cm-pod1 으로 pod 하나 작성할 때 configmap 을 설정한다. apply 후
		# kubectl exec cm-pod1 -- env: 잘 적용되어 있는지 확인
	
		case 2) 볼륨기법. 볼륨으로 mount해서 configmap을 파일로 떨구기
		cm-1 폴더의 cm-pod2.yaml 참고
		
		# kubectl exec cm-pod2 -- cat /etc/config/[key]: mount 잘 되어있는지 확인
			[key]=LOG_LEVEL. key는 configmap 생성시 작성한 key 값이다. 그럼 해당 key의 value값을 확인할 수 있다.
			
		case 3) 볼륨기법. *.conf 파일을 연동하는 방법
		cm-1 폴더의 redis.conf, redis-cm-pod.yaml 참고
		
		# kubectl create cm redis-cm --from-file=redis.conf: cm 생성
		redis-cm-pod.yaml 로 pod 생성 후
		# kubectl exec redis-cm-pod -- ls /opt/redis-config: 확인

	- cm의 yaml 파일을 자동으로 만들어주고 실행하는 방법
		# kubectl create cm my-cm1 --from-literal=k8s-key=k8s-value --dry-run=client -o yaml: 실행된 것이 아닌 cm 설정파일을 yaml로 출력해줌.
		# kubectl create cm my-cm1 --from-literal=k8s-key=k8s-value --dry-run=client -o yaml > my-cm1.yaml 하면 떨궈줌
		# kubectl apply -f my-cm1.yaml
		# kubectl get cm: 확인할 수 있음.
		
	
	[실습: nginx를 proxy로 이용해서 통신, configmap]
	시나리오: 외부에서 ngix 80번 포트를 타고 nginx에서는 flask 5000번 포트로 준다음 flask의 내용을 외부에서 볼 수 있도록.
		nginx 80번 포트를 타기 위해 NodePort의 service를 생성해서 외부포트통신을 열어준다.
		nginx는 웹서버가 아닌 proxy로 사용
		
	1. nginx.conf를 저장한 configmap 생성
	2. multi-container pod
		1) nginx (80)
		2) webapp (flask (5000))
	3) NodePort 를 이용한 외부 접근 
	즉, 외부에서 30000(nodeport)번대 포트로 pod에 접근하고 pod의 nginx(80)로 들어온 다음 webapp(5000) 으로 들어오면 "hello world!" 출력

	cm-1 폴더의 webapp-conf.yaml, webapp-pod.yaml, webapp-svc.yaml 참고

	# kubectl get pod -o wide
	# curl [webapp-pod 의 IP]: 되는지 확인

	# kubectl get pod,svc,cm -o wide: 어느 노드에 붙었나 확인후 그 노드 IP로 외부에서 접속해보기
		pod/webapp-pod   2/2     Running   0          5m34s   10.111.156.91   k8s-node1   <none>           <none>
		service/frontend-svc   NodePort    10.96.4.251   <none>        80:31111/TCP   48s   app=webapp
		configmap/nginx-conf         1      15m
		
		> 192.168.56.101:3111
	
	- secret 생성
		# kubectl create secret generic my-pwd --from-literal=mypassword=k8spass#
			generic 은 value를 인코딩해줌
		# kubectl get secrets: 확인
		# kubectl get secrets my-pwd -o yaml: 인코딩 된 값 확인
	
	- yaml 자동생성으로 secret 만들기
		# kubectl create secret generic my-pwd2 --from-literal=mypassword=k8spass# --dry-run=client -o yaml > my-pwd2.yaml
		# kubectl apply -f my-pwd2.yaml

		
	[실습: scret]
	secret-1 폴더의 secret-pod1.yaml 참고
	
	apply 후
	# kubectl exec secret-pod1 -- env: 자동으로 decode 되는걸 볼 수 있다.
	
	[configMap + secret 실습]
		conf-sec 폴더의 cm-sec-pod.yaml 참고
		
		# kubectl create configmap cm-dev --from-literal SSH=false --from-literal=User=horim --dry-run=client -o yaml > cm-lab.yaml
		# kubectl apply -f cm-lab.yaml
		
		# kubectl create secret generic sec-dev --from-literal=Key=YnJhdm9teWxpZmU= --dry-run=client -o yaml > sec-lab.yaml
		# cat sec-lab.yaml: data.Key 를 YnJhdm9teWxpZmU= 이걸로 바꿔주자.
		# kubectl apply -f sec-lab.yaml
		
		pod 생성하고
		
		# kubectl exec -it cm-sec-pod -- env: 디코딩된 Key확인
		
	[실습: ingress]
	ingress-exam 폴더 참고
	
	1. deployment 생성: nginx-go-deployment.yaml 
	2. service 생성: nginx-go-svc.yaml
	3. secret 생성
		# vi /etc/hosts: 노드마다  nginx.k8s.com   goapp.k8s.com 로 다중 host명 지정
		# openssl genrsa -out server.key 2048
		# openssl req -new -x509 -key server.key -out server.cert -days 360 -subj /CN=nginx.k8s.com,goapp.k8s.com
		# kubectl create secret tls k8s-secret --cert=server.cert --key=server.key
	4. ingress 생성: nginx-go-ingress.yaml
		
	# kubectl get deploy,po,svc,ing -o wide: 전부 되는지 확인
	# curl goapp.k8s.com:[port 번호]
	
	* nginx 식별을 위해 index.html 수정 후 확인
		각 노드로 들어가서 /usr/share/nginx/html/ 경로에 index.html 파일 넣기
		# docker ps | grep nginx: container ID 얻기
		# docker cp index.html [컨테이너ID]:/usr/share/nginx/html/
		# curl goapp.k8s.com:[port 번호]: master node에서 확인

	
	
	
	
	# curl nginx.k8s.com:[port 번호]
	1:33:06